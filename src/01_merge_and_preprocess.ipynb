{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Turing Test: PyMongo Edition\n",
    "\n",
    "| **Notebook** | `01_merge.ipynb`                                               |\n",
    "|-------------:|----------------------------------------------------------------|\n",
    "|  **Purpose** | Integrate raw data from multiple sources into a common schema. |\n",
    "|     **Team** | John Johnson, Justin Minnion, Srinivas Pai                     |\n",
    "|   **Course** | INFO 607 \"Applied Database Technologies\"                       |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Prerequisites\n",
    "\n",
    "Prior to executing code within this Jupyter notebook, the following prerequisites must be met.\n",
    "\n",
    "- A MongoDB server is running and accessible by this notebook.\n",
    "- The initial loading of raw data (CSV and JSON files) has been performed using the included `utils.py` method `load_raw_data(...)` \n",
    "    - Note this function can be invoked from command-line:  \n",
    "      `>>> python main.py --load-data`\n",
    "- A Python environment is available and packages in requirements.txt (including their respective dependencies) have been installed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from Python standard library\n",
    "import math\n",
    "\n",
    "# local imports\n",
    "import utils\n",
    "from utils import TweetDB\n",
    "\n",
    "# pymongo\n",
    "import pymongo\n",
    "import pymongo.cursor\n",
    "from bson.objectid import ObjectId\n",
    "\n",
    "# other packages\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Options / Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE_DEFAULT = utils.CHUNK_SIZE_DEFAULT\n",
    "\n",
    "# Collection Names\n",
    "COLLECTION_RAW = utils.COLLECTION_NAMES['raw']\n",
    "COLLECTION_MERGED = utils.COLLECTION_NAMES['merged']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Make Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = utils.TweetDB()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Label source of data points"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the `raw` collection to add a data source for each tweet. Because this is non-destructive, we'll make the edit in-place to the existing documents."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Verified users  \n",
    "\n",
    "Data for these users were obtained from the Twitter API as nested JSON data. When compared to the CSV-format data from FiveThirtyEight, these tweets contain more fields and more data. We'll work to integrate the two data sources into one federated dataset, but to do so we'll need to apply different transformations to each sources' data. This added data source label will help streamline that.  \n",
    "\n",
    "Verified user tweets will be found based on:\n",
    " - Contain a top-level (in JSON hierarchy) field called \"`created_at`\".\n",
    " - Tweets from the FiveThirtyEight dataset do not contain this field.\n",
    "\n",
    "Verified user tweets will be modifed to:\n",
    " - Add new field \"`data_source`\" with value \"`verified`\"\n",
    "\n",
    "**_Note on Batching_**\n",
    "\n",
    "The method of batching is demonstrated here, as well. In order to prepare this task for future parallelization, as well as reducing the memory footprint required to execute this code in its proof-of-concept form on a single compute resource, we divide the update operation into batches.\n",
    "\n",
    "General approach for batching:\n",
    " 1. Create a query dictionary to identify which tweets to grab.\n",
    " 2. Apply the `TweetDB.query()` function using this query, but only retrieve the MongoDB \"`_id`\" field. The `_id` field is autogenerated by MongoDB and contains a unique, 12-byte, surrogate primary key for each document (tweet).\n",
    " 3. The `query()` function returns a PyMongo `cursor` object, which is effectively a lazy-evaluated Python generator. We could iterate over the cursor directly, but the cursor by default returns one document at a time, so we'd lose the ability to batch/paralellize. Instead, we can use a specialized Python `itertools` iterator to divide the cursor into batches.\n",
    " 4. Feed the returned cursor to the `utils.batched()` function. This wraps the original cursor generator into another generator (Python `itertools.islice`), but allows us to iterate over batches. It even handles the scenario where (unless batch size divides evenly into the total document size) the last batch will have fewer elements than the target batch size.\n",
    "\n",
    " We attempted a few other approaches to batching the PyMongo `cursor`, and considered the built-in `pymongo.collections.find_raw_batches()` function as well, but none provided the simplicity/readability of the Python `itertools.islice` approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets to be modified:  1,508,028\n"
     ]
    }
   ],
   "source": [
    "source_collection = COLLECTION_RAW\n",
    "dest_collection = COLLECTION_RAW    # in-place modification\n",
    "\n",
    "# setup query\n",
    "query_dict = {\n",
    "    'created_at': {         # look for field \"created_at\"\n",
    "        '$exists': True,    # ... and check if it is present in a record\n",
    "    },\n",
    "}\n",
    "\n",
    "# get a sense of how many tweets will be modified\n",
    "n_tweets: int = db.count_tweets_by_filter(\n",
    "    collection=source_collection,\n",
    "    query_dict=query_dict,\n",
    "    approximate=False\n",
    ")\n",
    "\n",
    "print(f\"Number of tweets to be modified:  {n_tweets:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of batches:   0%|          | 0/31 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of batches: 100%|██████████| 31/31 [10:20<00:00, 20.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time to complete: 0 days 00:10:20.473210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# make initial query to pull `_id` values\n",
    "return_fields = ['_id']\n",
    "\n",
    "tweet_id_cursor:pymongo.cursor.Cursor = db.query(\n",
    "    collection=source_collection,\n",
    "    query_dict=query_dict,\n",
    "    return_fields=return_fields\n",
    ")\n",
    "\n",
    "# setup dict for the update to be made\n",
    "update_dict = {\n",
    "    '$set': {\n",
    "        'data_source': 'verified',\n",
    "    },\n",
    "}\n",
    "\n",
    "# update in batches\n",
    "chunk_size = CHUNK_SIZE_DEFAULT\n",
    "n_chunks = math.ceil(n_tweets / chunk_size)\n",
    "\n",
    "time_start = pd.Timestamp.now()\n",
    "\n",
    "#   outer loop iterates over chunks (batches)\n",
    "for chunk in utils.batched(cursor=tweet_id_cursor, chunk_size=chunk_size, \n",
    "                           show_progress_bar=True, progress_bar_n_chunks=n_chunks):\n",
    "    # inner loop iterates over tweets (documents) within a chunk\n",
    "    for doc in chunk:\n",
    "        # `doc` is a dict with key=document field (str), value=value (Any)\n",
    "        #   example value for `doc`: \n",
    "        #       {'_id': ObjectId('6458645e09e423ae6e15d8e4')}\n",
    "        doc_query_dict = {'_id': doc['_id']}\n",
    "        doc_update_dict = update_dict\n",
    "\n",
    "        # make the update to this doc\n",
    "        db.update_tweets(\n",
    "            collection=dest_collection,\n",
    "            query_dict=doc_query_dict,\n",
    "            update_dict=doc_update_dict,\n",
    "            verbose=False   # reiterating the default value\n",
    "        )\n",
    "    # </inner loop>\n",
    "# </outer loop>\n",
    "\n",
    "time_end = pd.Timestamp.now()\n",
    "\n",
    "print(\"\\nTime to complete:\", str(time_end - time_start), end=\"\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of comparison, we also wanted to try that same operation without batching. We're interested to see whether MongoDB's internal optimization can accomplish this task without 1) taking longer than our batched approach, or 2) crashing the Jupyter kernel / host computer.\n",
    "\n",
    "First we'll unset the new field we just created, deleting it from the ~1.5 million tweets previously modified by the batched edit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_tweets: update was acknowledged \n",
      "\tnumber of tweets modified: 1508028 \n",
      "\tnumber of tweets matched:  1508028\n",
      "\n",
      "Time to complete: 0 days 00:00:26.444387\n"
     ]
    }
   ],
   "source": [
    "time_start = pd.Timestamp.now()\n",
    "\n",
    "# first remove the field we just added\n",
    "revert_update_dict = {\n",
    "    '$unset': {\n",
    "        'data_source': \"\",\n",
    "    },\n",
    "}\n",
    "\n",
    "db.update_tweets(\n",
    "    collection=dest_collection,\n",
    "    query_dict=query_dict,\n",
    "    update_dict=revert_update_dict,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "time_end = pd.Timestamp.now()\n",
    "\n",
    "print(\"\\nTime to complete:\", str(time_end - time_start))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, apply the same `update_dict` from the batched edit, but do so to the entire result of `query_dict` rather than batched subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_tweets: update was acknowledged \n",
      "\tnumber of tweets modified: 1508028 \n",
      "\tnumber of tweets matched:  1508028\n",
      "\n",
      "Time to complete: 0 days 00:00:24.221344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_start = pd.Timestamp.now()\n",
    "\n",
    "db.update_tweets(\n",
    "    collection=dest_collection,\n",
    "    query_dict=query_dict,\n",
    "    update_dict=update_dict,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "time_end = pd.Timestamp.now()\n",
    "\n",
    "print(\"\\nTime to complete:\", str(time_end - time_start), end=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, that operation seems to have worked a lot faster. It's possible the overhead of the many individual updates/queries outweights the benefits of memory footprint reduction.\n",
    "\n",
    "To summarize the results:\n",
    " - Both approaches applied identical edits to identical corpus of ~1.5 million tweets\n",
    " - Using the same computing environment:\n",
    "    - Batching/chunking manually required ~10 minutes to complete the operation\n",
    "    - Allowing PyMongo/MongoDB to handle the batching/chunking required ~0.5 minutes to complete the operation (1/20th the amount of time).\n",
    "    - Informal monitoring of RAM usage for both operations did not show any significant difference.\n",
    "\n",
    "For reference, the above tests were conducted with a Windows 10 PC equipped with an AMD Ryzen 7 5800X (8-core/16-thread) CPU and 64 GB RAM. The running MongoDB service did not exceed ~7.3 GB of RAM usage during these tests."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Troll users\n",
    "\n",
    "Data for these users were obtained from Twitter, researchers at Clemson University, and the website FiveThirtyEight. The data were provided in CSV files and did not contain a column for \"`created_at`\".  \n",
    "\n",
    "Troll user tweets will be found based on:\n",
    " - Do not contain a top-level (in JSON hierarchy) field called \"`created_at`\".\n",
    " - Tweets from the Twitter API (of verified users) do contain this field.\n",
    "\n",
    "Troll user tweets will be modifed to:\n",
    " - Add new field \"`data_source`\" with value \"`troll`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets to be modified:  2,946,207\n"
     ]
    }
   ],
   "source": [
    "source_collection = COLLECTION_RAW\n",
    "dest_collection = COLLECTION_RAW    # in-place modification\n",
    "\n",
    "# setup query\n",
    "query_dict = {\n",
    "    'created_at': {         # look for field \"created_at\"\n",
    "        '$exists': False,    # ... and check if it is NOT present in a record\n",
    "    },\n",
    "}\n",
    "\n",
    "# get a sense of how many tweets will be modified\n",
    "n_tweets: int = db.count_tweets_by_filter(\n",
    "    collection=source_collection,\n",
    "    query_dict=query_dict,\n",
    "    approximate=False\n",
    ")\n",
    "\n",
    "print(f\"Number of tweets to be modified:  {n_tweets:,}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll give the home-made batching approach another try, but based on the prior operation we're expecting this to take >20 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of batches: 100%|██████████| 59/59 [20:02<00:00, 20.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time to complete: 0 days 00:20:02.405691\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# make initial query to pull `_id` values\n",
    "return_fields = ['_id']\n",
    "\n",
    "tweet_id_cursor:pymongo.cursor.Cursor = db.query(\n",
    "    collection=source_collection,\n",
    "    query_dict=query_dict,\n",
    "    return_fields=return_fields\n",
    ")\n",
    "\n",
    "# setup dict for the update to be made\n",
    "update_dict = {\n",
    "    '$set': {\n",
    "        'data_source': 'troll',\n",
    "    },\n",
    "}\n",
    "\n",
    "# update in batches\n",
    "chunk_size = CHUNK_SIZE_DEFAULT\n",
    "n_chunks = math.ceil(n_tweets / chunk_size)\n",
    "\n",
    "time_start = pd.Timestamp.now()\n",
    "\n",
    "#   outer loop iterates over chunks (batches)\n",
    "for chunk in utils.batched(cursor=tweet_id_cursor, chunk_size=chunk_size, \n",
    "                           show_progress_bar=True, progress_bar_n_chunks=n_chunks):\n",
    "    # inner loop iterates over tweets (documents) within a chunk\n",
    "    for doc in chunk:\n",
    "        # `doc` is a dict with key=document field (str), value=value (Any)\n",
    "        #   example value for `doc`: \n",
    "        #       {'_id': ObjectId('6458645e09e423ae6e15d8e4')}\n",
    "        doc_query_dict = {'_id': doc['_id']}\n",
    "        doc_update_dict = update_dict\n",
    "\n",
    "        # make the update to this doc\n",
    "        db.update_tweets(\n",
    "            collection=dest_collection,\n",
    "            query_dict=doc_query_dict,\n",
    "            update_dict=doc_update_dict,\n",
    "            verbose=False   # reiterating the default value\n",
    "        )\n",
    "    # </inner loop>\n",
    "# </outer loop>\n",
    "\n",
    "time_end = pd.Timestamp.now()\n",
    "\n",
    "print(\"\\nTime to complete:\", str(time_end - time_start), end=\"\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this operation required about 20 minutes of run time to complete.\n",
    "\n",
    "This is a strong indication that the home-made batching approach we're using may not be efficient at this scale of data. That is to say, the additional overhead of batching and making multiple queries / updates to the dataset may be the rate-limiting step for our under-five-gigabyte dataset scale.\n",
    "\n",
    "We gave the batching one additional try, let's also give the PyMongo-managed batching a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_tweets: update was acknowledged \n",
      "\tnumber of tweets modified: 2946207 \n",
      "\tnumber of tweets matched:  2946207\n",
      "\n",
      "Time to complete: 0 days 00:00:30.920849\n"
     ]
    }
   ],
   "source": [
    "time_start = pd.Timestamp.now()\n",
    "\n",
    "# first remove the field we just added\n",
    "revert_update_dict = {\n",
    "    '$unset': {\n",
    "        'data_source': \"\",\n",
    "    },\n",
    "}\n",
    "\n",
    "db.update_tweets(\n",
    "    collection=dest_collection,\n",
    "    query_dict=query_dict,\n",
    "    update_dict=revert_update_dict,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "time_end = pd.Timestamp.now()\n",
    "\n",
    "print(\"\\nTime to complete:\", str(time_end - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_tweets: update was acknowledged \n",
      "\tnumber of tweets modified: 2946207 \n",
      "\tnumber of tweets matched:  2946207\n",
      "\n",
      "Time to complete: 0 days 00:00:32.437216"
     ]
    }
   ],
   "source": [
    "time_start = pd.Timestamp.now()\n",
    "\n",
    "db.update_tweets(\n",
    "    collection=dest_collection,\n",
    "    query_dict=query_dict,\n",
    "    update_dict=update_dict,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "time_end = pd.Timestamp.now()\n",
    "\n",
    "print(\"\\nTime to complete:\", str(time_end - time_start), end=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again as expected, the approach that doesn't use our homemade batching completed in significantly less time. Unexpected, however, was that applying this modification (adding a new field) for roughly twice the number of tweets (2.95 million troll tweets versus 1.51 million authentic tweets) only increased the operation's time by one third (32 seconds versus 24 seconds), rather than ~doubling the time. Clearly MongoDB has some optimization taking place under the hood where the size of this operation does not linearly increase the time required.\n",
    "\n",
    "We've still demonstrated it is possible to break apart an operation like this into chunks, but in the interest of demonstrating more PyMongo concepts, we will likely apply the PyMongo-managed batching approach from here onward."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Filter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Authentic Tweets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 - Extract Columns of Interest\n",
    "\n",
    "We won't use all of the authentic tweet fields exported from the Twitter API, and we need to work towards aligning the raw data fields of authentic tweets with the column features of the troll data CSV files.\n",
    "\n",
    "To do this, we'll create a new collection \"merged\" and copy selected tweet data from collection \"raw\" to there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new collection\n",
    "db.create_collection(\"merged\", overwrite_old_collection=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_keep_EDA = [\n",
    "    'author_id',\n",
    "    'created_at',\n",
    "    'id',\n",
    "    'text',\n",
    "    'lang',\n",
    "    'referenced_tweets',\n",
    "    'public_metrics.retweet_count', \n",
    "    'public_metrics.reply_count', \n",
    "    'public_metrics.like_count', \n",
    "    'public_metrics.quote_count',\n",
    "    'author.location', \n",
    "    'author.name', \n",
    "    'author.username', \n",
    "    'author.public_metrics.followers_count',\n",
    "    'author.public_metrics.following_count', \n",
    "    'author.entities.url.urls', \n",
    "    'author.created_at',\n",
    "    'author.verified', \n",
    "    'context_annotations', \n",
    "    'entities.annotations', \n",
    "    'entities.mentions',\n",
    "    'entities.hashtags', \n",
    "    'entities.urls',\n",
    "    'data_source'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets to be copied:  1,508,028\n"
     ]
    }
   ],
   "source": [
    "source_collection = COLLECTION_RAW\n",
    "dest_collection = COLLECTION_MERGED\n",
    "\n",
    "# setup query\n",
    "query_dict = {\n",
    "    'data_source':          # look for field \"data_source\", created in prior section\n",
    "        'verified'          # ... and check for verified tweets\n",
    "}\n",
    "\n",
    "# get a sense of how many tweets will be modified\n",
    "n_tweets: int = db.count_tweets_by_filter(\n",
    "    collection=source_collection,\n",
    "    query_dict=query_dict,\n",
    "    approximate=False\n",
    ")\n",
    "\n",
    "print(f\"Number of tweets to be copied:  {n_tweets:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time to complete: 0 days 00:01:21.877496"
     ]
    }
   ],
   "source": [
    "time_start = pd.Timestamp.now()\n",
    "\n",
    "db.copy_tweets(\n",
    "    source_collection=source_collection,\n",
    "    source_query_dict=query_dict,\n",
    "    dest_collection=dest_collection,\n",
    "    source_fields=cols_keep_EDA\n",
    ")\n",
    "\n",
    "time_end = pd.Timestamp.now()\n",
    "print(\"\\nTime to complete:\", str(time_end - time_start), end=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That copy_tweets operation worked well, but in the past project we also applied the function `pandas.json_normalize()` as a means of *flattening* the nested JSON hierarchy to a flat hierarchy. To align the JSON data with CSV data, flattening is necessary.\n",
    "\n",
    "This could also be accomplished by a field-to-field mapping, individually assigning new fields at the top hierarchy level from fields at nested hierarchy levels. We're essentially doing the same thing, but have automated it a bit. The terminology we'll use for this operation is to \"promote\" the nested fields, in reference to their movement to the top hierarchy level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_tweets: update was acknowledged \n",
      "\tnumber of tweets modified: 1508028 \n",
      "\tnumber of tweets matched:  1508028\n",
      "\n",
      "Time to complete: 0 days 00:00:58.051244\n"
     ]
    }
   ],
   "source": [
    "# nested fields all contain a \".\" (period/dot) character, so we can filter them based on that\n",
    "nested_fields = [field for field in cols_keep_EDA if (\".\" in field)]\n",
    "\n",
    "# same query dict\n",
    "query_dict = {\n",
    "    'data_source':          # look for field \"data_source\", created in prior section\n",
    "        'verified'          # ... and check for verified tweets\n",
    "}\n",
    "\n",
    "time_start = pd.Timestamp.now()\n",
    "\n",
    "# promote the nested fields\n",
    "db.promote_fields(\n",
    "    collection=dest_collection,\n",
    "    query_dict=query_dict,\n",
    "    field_list=nested_fields\n",
    ")\n",
    "\n",
    "time_end = pd.Timestamp.now()\n",
    "print(\"\\nTime to complete:\", str(time_end - time_start))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 - Rename fields (columns) to align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename mapping\n",
    "#   past notebook had fields using dot notation, we're using double underscore from prior step\n",
    "authentic_rename_dict = {\n",
    "    \"author_id\": \"external_author_id\", \n",
    "    \"created_at\": \"publish_date\", \n",
    "    \"text\": \"content\",\n",
    "    \"lang\": \"language\", \n",
    "    \"author__location\": \"region\", \n",
    "    \"author__username\": \"author\",\n",
    "    \"author__name\": \"full_name\",\n",
    "    \"author__public_metrics__followers_count\": \"followers\",\n",
    "    \"author__public_metrics__following_count\": \"following\",\n",
    "    \"id\": \"tweet_id\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets to be modified:  1,508,028\n"
     ]
    }
   ],
   "source": [
    "source_collection = COLLECTION_MERGED\n",
    "dest_collection = COLLECTION_MERGED     # in-place change\n",
    "\n",
    "# setup query\n",
    "query_dict = {\n",
    "    'data_source':          # look for field \"data_source\", created in prior section\n",
    "        'verified'          # ... and check for verified tweets\n",
    "}\n",
    "\n",
    "# get a sense of how many tweets will be modified\n",
    "n_tweets: int = db.count_tweets_by_filter(\n",
    "    collection=source_collection,\n",
    "    query_dict=query_dict,\n",
    "    approximate=False\n",
    ")\n",
    "\n",
    "print(f\"Number of tweets to be modified:  {n_tweets:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_tweets: update was acknowledged \n",
      "\tnumber of tweets modified: 1508028 \n",
      "\tnumber of tweets matched:  1508028\n",
      "\n",
      "Time to complete: 0 days 00:00:50.500829"
     ]
    }
   ],
   "source": [
    "# setup dict for the update to be made\n",
    "update_dict = {\n",
    "    '$rename': authentic_rename_dict,\n",
    "}\n",
    "\n",
    "time_start = pd.Timestamp.now()\n",
    "\n",
    "db.update_tweets(\n",
    "    collection=dest_collection,\n",
    "    query_dict=query_dict,\n",
    "    update_dict=update_dict,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "time_end = pd.Timestamp.now()\n",
    "\n",
    "print(\"\\nTime to complete:\", str(time_end - time_start), end=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Troll Tweets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 - Filter for \"*Only English language tweets*\" and Extract Columns of Interest\n",
    "\n",
    "One step from the past project was to retain (from the troll tweets) only the English language tweets, as indicated by the field \"language\" containing the value \"English\".\n",
    "\n",
    "We haven't yet moved our raw troll tweets from the \"raw\" collection to the \"merged\" collection, so will perform this filter step as we move them.\n",
    "\n",
    "Finally, we'll also filter down some of the fields from CSV columns to exclude a small number of fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [\n",
    "    'external_author_id',\n",
    "    'author',\n",
    "    'content',\n",
    "    'region',\n",
    "    'language',\n",
    "    'publish_date',\n",
    "    'following',\n",
    "    'followers',\n",
    "    'updates',\n",
    "    'post_type',\n",
    "    'retweet',\n",
    "    'account_category',\n",
    "    'tweet_id',\n",
    "    'tco1_step1',\n",
    "    'data_source'       # added this, not in past project\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets to be copied:  2,116,867\n"
     ]
    }
   ],
   "source": [
    "source_collection = COLLECTION_RAW\n",
    "dest_collection = COLLECTION_MERGED\n",
    "\n",
    "# setup query\n",
    "query_dict = {\n",
    "    'data_source': 'troll',\n",
    "    'language': 'English'\n",
    "}\n",
    "\n",
    "# get a sense of how many tweets will be modified\n",
    "n_tweets: int = db.count_tweets_by_filter(\n",
    "    collection=source_collection,\n",
    "    query_dict=query_dict,\n",
    "    approximate=False\n",
    ")\n",
    "\n",
    "print(f\"Number of tweets to be copied:  {n_tweets:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time to complete: 0 days 00:01:41.307964"
     ]
    }
   ],
   "source": [
    "time_start = pd.Timestamp.now()\n",
    "\n",
    "db.copy_tweets(\n",
    "    source_collection=source_collection,\n",
    "    source_query_dict=query_dict,\n",
    "    dest_collection=dest_collection,\n",
    "    source_fields=cols_to_keep\n",
    ")\n",
    "\n",
    "time_end = pd.Timestamp.now()\n",
    "print(\"\\nTime to complete:\", str(time_end - time_start), end=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 - Rename columns to align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename mapping\n",
    "#   past notebook had fields using dot notation, we're using double underscore from prior step\n",
    "troll_rename_dict = {\n",
    "    \"retweet\": \"is_retweet\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets to be modified:  2,116,867\n"
     ]
    }
   ],
   "source": [
    "source_collection = COLLECTION_MERGED\n",
    "dest_collection = COLLECTION_MERGED     # in-place change\n",
    "\n",
    "# setup query\n",
    "query_dict = {\n",
    "    'data_source':          # look for field \"data_source\", created in prior section\n",
    "        'troll'          # ... and check for verified tweets\n",
    "}\n",
    "\n",
    "# get a sense of how many tweets will be modified\n",
    "n_tweets: int = db.count_tweets_by_filter(\n",
    "    collection=source_collection,\n",
    "    query_dict=query_dict,\n",
    "    approximate=False\n",
    ")\n",
    "\n",
    "print(f\"Number of tweets to be modified:  {n_tweets:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_tweets: update was acknowledged \n",
      "\tnumber of tweets modified: 2116867 \n",
      "\tnumber of tweets matched:  2116867\n",
      "\n",
      "Time to complete: 0 days 00:00:24.635927"
     ]
    }
   ],
   "source": [
    "# setup dict for the update to be made\n",
    "update_dict = {\n",
    "    '$rename': troll_rename_dict,\n",
    "}\n",
    "\n",
    "time_start = pd.Timestamp.now()\n",
    "\n",
    "db.update_tweets(\n",
    "    collection=dest_collection,\n",
    "    query_dict=query_dict,\n",
    "    update_dict=update_dict,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "time_end = pd.Timestamp.now()\n",
    "\n",
    "print(\"\\nTime to complete:\", str(time_end - time_start), end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info607_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
