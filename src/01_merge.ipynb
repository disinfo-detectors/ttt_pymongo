{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Turing Test: PyMongo Edition\n",
    "\n",
    "| **Notebook** | `01_merge.ipynb`                                               |\n",
    "|-------------:|----------------------------------------------------------------|\n",
    "|  **Purpose** | Integrate raw data from multiple sources into a common schema. |\n",
    "|     **Team** | John Johnson, Justin Minnion, Srinivas Pai                     |\n",
    "|   **Course** | INFO 607 \"Applied Database Technologies\"                       |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Prerequisites\n",
    "\n",
    "Prior to executing code within this Jupyter notebook, the following prerequisites must be met.\n",
    "\n",
    "- A MongoDB server is running and accessible by this notebook.\n",
    "- The initial loading of raw data (CSV and JSON files) has been performed using the included `utils.py` method `load_raw_data(...)` \n",
    "    - Note this function can be invoked from command-line:  \n",
    "      `>>> python main.py --load-data`\n",
    "- A Python environment is available and packages in requirements.txt (including their respective dependencies) have been installed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from Python standard library\n",
    "import math\n",
    "\n",
    "# local imports\n",
    "import utils\n",
    "from utils import TweetDB\n",
    "\n",
    "# pymongo\n",
    "import pymongo\n",
    "import pymongo.cursor\n",
    "from bson.objectid import ObjectId\n",
    "\n",
    "# other packages\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Options / Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE_DEFAULT = utils.CHUNK_SIZE_DEFAULT\n",
    "\n",
    "# Collection Names\n",
    "COLLECTION_RAW = utils.COLLECTION_NAMES['raw']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Make Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = utils.TweetDB()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Label source of data points"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the `raw` collection to add a data source for each tweet. Because this is non-destructive, we'll make the edit in-place to the existing documents."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Verified users  \n",
    "\n",
    "Data for these users were obtained from the Twitter API as nested JSON data. When compared to the CSV-format data from FiveThirtyEight, these tweets contain more fields and more data. We'll work to integrate the two data sources into one federated dataset, but to do so we'll need to apply different transformations to each sources' data. This added data source label will help streamline that.  \n",
    "\n",
    "Verified user tweets will be found based on:\n",
    " - Contain a top-level (in JSON hierarchy) field called \"`created_at`\".\n",
    " - Tweets from the FiveThirtyEight dataset do not contain this field.\n",
    "\n",
    "Verified user tweets will be modifed to:\n",
    " - Add new field \"`data_source`\" with value \"`verified`\"\n",
    "\n",
    "**_Note on Batching_**\n",
    "\n",
    "The method of batching is demonstrated here, as well. In order to prepare this task for future parallelization, as well as reducing the memory footprint required to execute this code in its proof-of-concept form on a single compute resource, we divide the update operation into batches.\n",
    "\n",
    "General approach for batching:\n",
    " 1. Create a query dictionary to identify which tweets to grab.\n",
    " 2. Apply the `TweetDB.query()` function using this query, but only retrieve the MongoDB \"`_id`\" field. The `_id` field is autogenerated by MongoDB and contains a unique, 12-byte, surrogate primary key for each document (tweet).\n",
    " 3. The `query()` function returns a PyMongo `cursor` object, which is effectively a lazy-evaluated Python generator. We could iterate over the cursor directly, but the cursor by default returns one document at a time, so we'd lose the ability to batch/paralellize. Instead, we can use a specialized Python `itertools` iterator to divide the cursor into batches.\n",
    " 4. Feed the returned cursor to the `utils.batched()` function. This wraps the original cursor generator into another generator (Python `itertools.islice`), but allows us to iterate over batches. It even handles the scenario where (unless batch size divides evenly into the total document size) the last batch will have fewer elements than the target batch size.\n",
    "\n",
    " We attempted a few other approaches to batching the PyMongo `cursor`, and considered the built-in `pymongo.collections.find_raw_batches()` function as well, but none provided the simplicity/readability of the Python `itertools.islice` approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets to be modified:  1,508,028\n"
     ]
    }
   ],
   "source": [
    "source_collection = COLLECTION_RAW\n",
    "dest_collection = COLLECTION_RAW    # in-place modification\n",
    "\n",
    "# setup query\n",
    "query_dict = {\n",
    "    'created_at': {         # look for field \"created_at\"\n",
    "        '$exists': True,    # ... and check if it is present in a record\n",
    "    },\n",
    "}\n",
    "\n",
    "# get a sense of how many tweets will be modified\n",
    "n_tweets: int = db.count_tweets_by_filter(\n",
    "    collection=source_collection,\n",
    "    query_dict=query_dict,\n",
    "    approximate=False\n",
    ")\n",
    "\n",
    "print(f\"Number of tweets to be modified:  {n_tweets:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of batches: 100%|██████████| 31/31 [09:50<00:00, 19.03s/it]\n"
     ]
    }
   ],
   "source": [
    "# make initial query to pull `_id` values\n",
    "return_fields = ['_id']\n",
    "\n",
    "tweet_id_cursor:pymongo.cursor.Cursor = db.query(\n",
    "    collection=source_collection,\n",
    "    query_dict=query_dict,\n",
    "    return_fields=return_fields\n",
    ")\n",
    "\n",
    "# setup dict for the update to be made\n",
    "update_dict = {\n",
    "    '$set': {\n",
    "        'data_source': 'verified',\n",
    "    },\n",
    "}\n",
    "\n",
    "# update in batches\n",
    "chunk_size = CHUNK_SIZE_DEFAULT\n",
    "n_chunks = math.ceil(n_tweets / chunk_size)\n",
    "\n",
    "#   outer loop iterates over chunks (batches)\n",
    "for chunk in utils.batched(cursor=tweet_id_cursor, chunk_size=chunk_size, \n",
    "                           show_progress_bar=True, progress_bar_n_chunks=n_chunks):\n",
    "    # inner loop iterates over tweets (documents) within a chunk\n",
    "    for doc in chunk:\n",
    "        # `doc` is a dict with key=document field (str), value=value (Any)\n",
    "        #   example value for `doc`: \n",
    "        #       {'_id': ObjectId('6458645e09e423ae6e15d8e4')}\n",
    "        doc_query_dict = {'_id': doc['_id']}\n",
    "        doc_update_dict = update_dict\n",
    "\n",
    "        # make the update to this doc\n",
    "        db.update_tweets(\n",
    "            collection=dest_collection,\n",
    "            query_dict=doc_query_dict,\n",
    "            update_dict=doc_update_dict,\n",
    "            verbose=False   # reiterating the default value\n",
    "        )\n",
    "    # </inner loop>\n",
    "# </outer loop>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of comparison, we also wanted to try that same operation without batching. We're interested to see whether MongoDB's internal optimization can accomplish this task without 1) taking longer than our batched approach, or 2) crashing the Jupyter kernel / host computer.\n",
    "\n",
    "First we'll unset the new field we just created, deleting it from the ~1.5 million tweets previously modified by the batched edit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_tweets: update was acknowledged \tnumber of tweets modified: 1508028 \tnumber of tweets matched:  1508028\n"
     ]
    }
   ],
   "source": [
    "# first remove the field we just added\n",
    "revert_update_dict = {\n",
    "    '$unset': {\n",
    "        'data_source': \"\",\n",
    "    },\n",
    "}\n",
    "\n",
    "db.update_tweets(\n",
    "    collection=dest_collection,\n",
    "    query_dict=query_dict,\n",
    "    update_dict=revert_update_dict,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, apply the same `update_dict` from the batched edit, but do so to the entire result of `query_dict` rather than batched subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_tweets: update was acknowledged \tnumber of tweets modified: 1508028 \tnumber of tweets matched:  1508028\n"
     ]
    }
   ],
   "source": [
    "db.update_tweets(\n",
    "    collection=dest_collection,\n",
    "    query_dict=query_dict,\n",
    "    update_dict=update_dict,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, that operation seems to have worked a lot faster. It's possible the overhead of the many individual updates/queries outweights the benefits of memory footprint reduction.\n",
    "\n",
    "To summarize the results:\n",
    " - Both approaches applied identical edits to identical corpus of ~1.5 million tweets\n",
    " - Using the same computing environment:\n",
    "    - Batching/chunking manually required ~10 minutes to complete the operation\n",
    "    - Allowing PyMongo/MongoDB to handle the batching/chunking required ~0.5 minutes to complete the operation (1/20th the amount of time).\n",
    "    - Informal monitoring of RAM usage for both operations did not show any significant difference.\n",
    "\n",
    "For reference, the above tests were conducted with a Windows 10 PC equipped with an AMD Ryzen 7 5800X (8-core/16-thread) CPU and 64 GB RAM. The running MongoDB service did not exceed ~7.3 GB of RAM usage during these tests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info607_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
